{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching (::Flux.RNNCell{var\"#13#14\", Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Matrix{Float32}})(::Matrix{Float32}, ::Float64)\nClosest candidates are:\n  (::Flux.RNNCell{F, I, H, V, <:AbstractMatrix{T}})(::Any, !Matched::Union{AbstractVector{T}, AbstractMatrix{T}, OneHotArrays.OneHotArray}) where {F, I, H, V, T} at ~/.julia/packages/Flux/ZdbJr/src/layers/recurrent.jl:203",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::Flux.RNNCell{var\"#13#14\", Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Matrix{Float32}})(::Matrix{Float32}, ::Float64)\n",
      "Closest candidates are:\n",
      "  (::Flux.RNNCell{F, I, H, V, <:AbstractMatrix{T}})(::Any, !Matched::Union{AbstractVector{T}, AbstractMatrix{T}, OneHotArrays.OneHotArray}) where {F, I, H, V, T} at ~/.julia/packages/Flux/ZdbJr/src/layers/recurrent.jl:203\n",
      "\n",
      "Stacktrace:\n",
      "  [1] (::Flux.Recur{Flux.RNNCell{var\"#13#14\", Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Matrix{Float32}}, Matrix{Float32}})(x::Float64)\n",
      "    @ Flux ~/.julia/packages/Flux/ZdbJr/src/layers/recurrent.jl:134\n",
      "  [2] iterate\n",
      "    @ ./generator.jl:47 [inlined]\n",
      "  [3] _collect\n",
      "    @ ./array.jl:807 [inlined]\n",
      "  [4] collect_similar\n",
      "    @ ./array.jl:716 [inlined]\n",
      "  [5] map\n",
      "    @ ./abstractarray.jl:2933 [inlined]\n",
      "  [6] broadcasted(f::Flux.Recur{Flux.RNNCell{var\"#13#14\", Matrix{Float32}, Matrix{Float32}, Vector{Float32}, Matrix{Float32}}, Matrix{Float32}}, args::Vector{Float64})\n",
      "    @ Flux ~/.julia/packages/Flux/ZdbJr/src/deprecations.jl:24\n",
      "  [7] eval_model(x::Vector{Float64})\n",
      "    @ Main ~/Documents/radarSim/fluxRNN.ipynb:25\n",
      "  [8] loss(x::Vector{Float64}, y::Float64)\n",
      "    @ Main ~/Documents/radarSim/fluxRNN.ipynb:30\n",
      "  [9] _broadcast_getindex_evalf\n",
      "    @ ./broadcast.jl:670 [inlined]\n",
      " [10] _broadcast_getindex\n",
      "    @ ./broadcast.jl:643 [inlined]\n",
      " [11] getindex\n",
      "    @ ./broadcast.jl:597 [inlined]\n",
      " [12] copy\n",
      "    @ ./broadcast.jl:899 [inlined]\n",
      " [13] materialize(bc::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1}, Nothing, typeof(loss), Tuple{Vector{Vector{Float64}}, Vector{Float64}}})\n",
      "    @ Base.Broadcast ./broadcast.jl:860\n",
      " [14] top-level scope\n",
      "    @ ~/Documents/radarSim/fluxRNN.ipynb:37"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "using Flux: @epochs\n",
    "\n",
    "num_samples = 1000\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "function generate_data(num_samples)\n",
    "    train_data = [rand(1.0:10.0, rand(2:7)) for i in 1:num_samples]\n",
    "    train_labels = (v -> sum(v)).(train_data)\n",
    "  \n",
    "    test_data = 2 .* train_data\n",
    "    test_labels = 2 .* train_labels\n",
    "  \n",
    "    train_data, train_labels, test_data, test_labels\n",
    "end\n",
    "  \n",
    "  \n",
    "# generate our test data with the data generation function from above\n",
    "train_data, train_labels, test_data, test_labels = generate_data(num_samples)\n",
    "simple_rnn = Flux.RNN(1, 1, (x -> x))\n",
    "\n",
    "function eval_model(x)\n",
    "  out = simple_rnn.(x)[end]\n",
    "  Flux.reset!(simple_rnn)\n",
    "  out\n",
    "end\n",
    "\n",
    "loss(x, y) = abs(sum((eval_model(x) .- y)))\n",
    "\n",
    "ps = Flux.params(simple_rnn)\n",
    "\n",
    "# use the ADAM optimizer. It's a pretty good one!\n",
    "opt = Flux.ADAM()\n",
    "\n",
    "println(\"Training loss before = \", sum(loss.(train_data, train_labels)))\n",
    "println(\"Test loss before = \", sum(loss.(test_data, test_labels)))\n",
    "\n",
    "# callback function during training\n",
    "evalcb() = @show(sum(loss.(test_data, test_labels)))\n",
    "\n",
    "@epochs num_epochs Flux.train!(loss, ps, zip(train_data, train_labels), opt, cb = Flux.throttle(evalcb, 1))\n",
    "\n",
    "# after training, evaluate the loss\n",
    "println(\"Test loss after = \", sum(loss.(test_data, test_labels)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
