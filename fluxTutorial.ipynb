{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Julia 1.7+, this will prompt if neccessary to install everything, including CUDA:\n",
    "using Flux, Statistics, ProgressMeter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:\n",
    "noisy = rand(Float32, 2, 1000)                                    # 2×1000 Matrix{Float32}\n",
    "truth = [xor(col[1]>0.5, col[2]>0.5) for col in eachcol(noisy)]   # 1000-element Vector{Bool}\n",
    "\n",
    "# Define our model, a multi-layer perceptron with one hidden layer of size 3:\n",
    "model = Chain(\n",
    "    Dense(2 => 3, tanh),   # activation function inside layer\n",
    "    BatchNorm(3),\n",
    "    Dense(3 => 2),\n",
    "    softmax) |> gpu        # move model to GPU, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model encapsulates parameters, randomly initialised. Its initial output is:\n",
    "out1 = model(noisy |> gpu) |> cpu                                 # 2×1000 Matrix{Float32}\n",
    "\n",
    "# To train the model, we use batches of 64 samples, and one-hot encoding:\n",
    "target = Flux.onehotbatch(truth, [true, false])                   # 2×1000 OneHotMatrix\n",
    "loader = Flux.DataLoader((noisy, target) |> gpu, batchsize=64, shuffle=true);\n",
    "# 16-element DataLoader with first element: (2×64 Matrix{Float32}, 2×64 OneHotMatrix)\n",
    "\n",
    "pars = Flux.params(model)  # contains references to arrays in model\n",
    "opt = Flux.Adam(0.01)      # will store optimiser momentum, etc.\n",
    "\n",
    "# Training loop, using the whole data set 1000 times:\n",
    "losses = []\n",
    "@showprogress for epoch in 1:1_000\n",
    "    for (x, y) in loader\n",
    "        loss, grad = Flux.withgradient(pars) do\n",
    "            # Evaluate model and loss inside gradient context:\n",
    "            y_hat = model(x)\n",
    "            Flux.crossentropy(y_hat, y)\n",
    "        end\n",
    "        Flux.update!(opt, pars, grad)\n",
    "        push!(losses, loss)  # logging, outside gradient context\n",
    "    end\n",
    "end\n",
    "\n",
    "pars  # parameters, momenta and output have all changed\n",
    "opt\n",
    "out2 = model(noisy |> gpu) |> cpu  # first row is prob. of true, second row p(false)\n",
    "\n",
    "mean((out2[1,:] .> 0.5) .== truth)  # accuracy 94% so far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots  # to draw the above figure\n",
    "\n",
    "p_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=\"True classification\", legend=false)\n",
    "p_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=out1[1,:], title=\"Untrained network\", label=\"\", clims=(0,1))\n",
    "p_done = scatter(noisy[1,:], noisy[2,:], zcolor=out2[1,:], title=\"Trained network\", legend=false)\n",
    "\n",
    "plot(p_true, p_raw, p_done, layout=(1,3), size=(1000,330))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(losses; xaxis=(:log10, \"iteration\"),\n",
    "    yaxis=\"loss\", label=\"per batch\")\n",
    "n = length(loader)\n",
    "plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),\n",
    "    label=\"epoch mean\", dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
