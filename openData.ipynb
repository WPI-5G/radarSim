{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "using Flux\n",
    "using Statistics\n",
    "using ProgressMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load \"Training.jld2\" inputs;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load \"Targets.jld2\"  actuals;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000-element Vector{Float64}:\n",
       " 2.3908511486970712e-9\n",
       " 2.436013828157643e-9\n",
       " 2.488448322654356e-9\n",
       " 2.5461683866947014e-9\n",
       " 2.6061683790637417e-9\n",
       " 2.664820124211142e-9\n",
       " 2.718154819959127e-9\n",
       " 2.7622417315175367e-9\n",
       " 2.7933285545808926e-9\n",
       " 2.8080158022726244e-9\n",
       " â‹®\n",
       " 4.0714169026759647e-13\n",
       " 3.7295985458878003e-13\n",
       " 3.366960978134348e-13\n",
       " 2.8178885545304874e-13\n",
       " 2.1419920234758155e-13\n",
       " 1.4220594657073096e-13\n",
       " 8.898707432537348e-14\n",
       " 5.6576979018973334e-14\n",
       " 2.1029735140690506e-14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = inputs[2,:];\n",
    "#input = reshape(input,:,100);\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "538100.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = rand(1:length(inputs[:,1]))\n",
    "testSet = inputs[idx,:];\n",
    "testSet = reshape(testSet,:,100);\n",
    "testTarg = actuals[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = actuals[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    RNN(100,1, NNlib.relu)\n",
    ") |> gpu;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.reset!(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8955161e11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# out1=-1\n",
    "# for i in eachrow(testSet)\n",
    "#     out1 = model(i |> gpu) |> cpu\n",
    "# end\n",
    "loss(testSet, testTarg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evalcb (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Flux.reset!(model)\n",
    "function eval_model(x)\n",
    "    out1=-1\n",
    "    for i in eachrow(x)\n",
    "        out1 = model(i |> gpu) |> cpu\n",
    "    end\n",
    "    out1\n",
    "end\n",
    "\n",
    "loss(x, y) = Flux.Losses.mse(eval_model(x), y)\n",
    "\n",
    "ps = Flux.params(model)\n",
    "\n",
    "opt = Flux.ADAM();\n",
    "\n",
    "evalcb() = @show(sum(loss.(testSet, testTarg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching eachrow(::Float64)\nClosest candidates are:\n  eachrow(!Matched::AbstractVecOrMat) at abstractarraymath.jl:550",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching eachrow(::Float64)\n",
      "Closest candidates are:\n",
      "  eachrow(!Matched::AbstractVecOrMat) at abstractarraymath.jl:550\n",
      "\n",
      "Stacktrace:\n",
      "  [1] macro expansion\n",
      "    @ ~/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:0 [inlined]\n",
      "  [2] _pullback(ctx::Zygote.Context{true}, f::typeof(eachrow), args::Float64)\n",
      "    @ Zygote ~/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:9\n",
      "  [3] _pullback\n",
      "    @ ~/Documents/radarSim/openData.ipynb:4 [inlined]\n",
      "  [4] _pullback(ctx::Zygote.Context{true}, f::typeof(eval_model), args::Float64)\n",
      "    @ Zygote ~/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:0\n",
      "  [5] _pullback\n",
      "    @ ~/Documents/radarSim/openData.ipynb:10 [inlined]\n",
      "  [6] _pullback(::Zygote.Context{true}, ::typeof(loss), ::Float64, ::Float64)\n",
      "    @ Zygote ~/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:0\n",
      "  [7] _apply(::Function, ::Vararg{Any})\n",
      "    @ Core ./boot.jl:816\n",
      "  [8] adjoint\n",
      "    @ ~/.julia/packages/Zygote/SmJK6/src/lib/lib.jl:203 [inlined]\n",
      "  [9] _pullback\n",
      "    @ ~/.julia/packages/ZygoteRules/AIbCs/src/adjoint.jl:65 [inlined]\n",
      " [10] _pullback\n",
      "    @ ~/.julia/packages/Flux/ZdbJr/src/optimise/train.jl:143 [inlined]\n",
      " [11] _pullback(::Zygote.Context{true}, ::Flux.Optimise.var\"#37#40\"{typeof(loss), Tuple{Float64, Float64}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/SmJK6/src/compiler/interface2.jl:0\n",
      " [12] pullback(f::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/SmJK6/src/compiler/interface.jl:384\n",
      " [13] withgradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "    @ Zygote ~/.julia/packages/Zygote/SmJK6/src/compiler/interface.jl:132\n",
      " [14] macro expansion\n",
      "    @ ~/.julia/packages/Flux/ZdbJr/src/optimise/train.jl:142 [inlined]\n",
      " [15] macro expansion\n",
      "    @ ~/.julia/packages/ProgressLogging/6KXlp/src/ProgressLogging.jl:328 [inlined]\n",
      " [16] train!(loss::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, data::Base.Iterators.Zip{Tuple{Matrix{Float64}, Vector{Float64}}}, opt::Adam; cb::Flux.Optimise.var\"#38#41\")\n",
      "    @ Flux.Optimise ~/.julia/packages/Flux/ZdbJr/src/optimise/train.jl:140\n",
      " [17] train!(loss::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, data::Base.Iterators.Zip{Tuple{Matrix{Float64}, Vector{Float64}}}, opt::Adam)\n",
      "    @ Flux.Optimise ~/.julia/packages/Flux/ZdbJr/src/optimise/train.jl:136\n",
      " [18] top-level scope\n",
      "    @ ~/Documents/radarSim/openData.ipynb:1"
     ]
    }
   ],
   "source": [
    "Flux.train!(loss, ps, zip(inputs,actuals), opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = length(inputs[:,1])\n",
    "# for x in 1:num_epochs\n",
    "#     Flux.train!(loss, ps, zip(inputs[x,:],actuals[x]), opt, cb = Flux.throttle(evalcb, 1))\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: test_data not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: test_data not defined\n",
      "\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "   @ ~/Documents/radarSim/openData.ipynb:1"
     ]
    }
   ],
   "source": [
    "println(\"Test loss after = \", sum(loss.(test_data, test_labels)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
